{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanggal = datetime.datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(os.path.join('../data/clean/data_clean_100k.res'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data['source_len']= d_data.source.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data_selected = d_data[d_data['source_len'] == MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data_selected.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = dict((idx, c) for idx, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\", 1))\n",
    "char2idx = dict((c, idx) for idx, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char[0] = '<UNK>'\n",
    "char2idx['<UNK>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_vectorizer(list_inputs, char_indices):\n",
    "    x = np.zeros((len([list_inputs]), MAX_LENGTH, len(char_indices)))\n",
    "    for i, input_ in enumerate([list_inputs]):\n",
    "        for t, char in enumerate(input_):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flag_space(sentence):\n",
    "    \n",
    "    no_space = []\n",
    "    flag_space = []\n",
    "    sentence = str(sentence)\n",
    "    for char in sentence: \n",
    "        if char != ' ':\n",
    "            no_space.append(char)\n",
    "            flag_space.append('0')\n",
    "        elif char == ' ':\n",
    "            flag_space[-1] = '1'\n",
    "            \n",
    "    no_space = ''.join(no_space)\n",
    "    flag_space = ''.join(flag_space)\n",
    "    \n",
    "    return flag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_vectorizer(list_inputs, char_indices):\n",
    "    x = np.zeros((len([list_inputs]), MAX_LENGTH, len(char_indices)))\n",
    "    for i, input_ in enumerate([list_inputs]):\n",
    "        for t, char in enumerate(input_):\n",
    "            try:\n",
    "                x[i, t, char_indices[char]] = 1\n",
    "            except:\n",
    "                x[i, t, 0] = 1\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_space_to_list(flag):\n",
    "    return np.array(list(flag)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "d_data_selected['flag_space'] = d_data_selected['target'].apply(get_flag_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "d_data_selected.loc[:, 'matrix'] = d_data_selected.loc[:, 'source'].apply(char_vectorizer, args=(char2idx,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "d_data_selected['flag_space_array'] = d_data_selected.flag_space.apply(flag_space_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "d_data_selected['flag_space_sum'] = d_data_selected.flag_space_array.apply(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, data):\n",
    "        row, col = data.shape\n",
    "        train = data.loc[:int(row*.8)]\n",
    "        test = data.loc[int(row*.8):]\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        self.lookup = {\n",
    "            'train': (train, len(train)),\n",
    "            'test': (test, len(test))\n",
    "        }\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    def set_split(self, split = 'train'):\n",
    "        self.data, self.length = self.lookup[split]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data.loc[index, 'matrix']\n",
    "        X = torch.Tensor(X).squeeze(0)\n",
    "        \n",
    "        y = np.array(list(self.data.loc[index, 'flag_space'])).astype(int)\n",
    "        y = torch.Tensor(y).squeeze(0)\n",
    "        \n",
    "        return {'x': X,\n",
    "               'y': y}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(len(char2idx), 256)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, input_, apply_sigmoid=False):\n",
    "        \n",
    "        y_pred, _ = self.lstm(input_)\n",
    "        y_pred, _ = self.lstm(input_, _)\n",
    "        y_pred = self.fc1(y_pred)\n",
    "        y_pred = self.fc2(y_pred)\n",
    "        y_pred = self.fc3(y_pred)\n",
    "        y_pred = self.fc4(y_pred)\n",
    "        y_pred = self.fc5(y_pred)\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            y_pred = torch.sigmoid(y_pred)\n",
    "        \n",
    "        y_pred = y_pred.squeeze(2)    \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    y_true = y_true.long().numpy()\n",
    "    y_pred = (y_pred > 0.5).long().numpy()\n",
    "    try:\n",
    "#         hamming_score = hamming_loss(y_true, y_pred)\n",
    "#         return 1 - hamming_score\n",
    "        return (y_true == y_pred).all(axis = 1).mean()\n",
    "    except:\n",
    "        print(\"y_true\", y_true, \"y_pred\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(d_data_selected)\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = 0.001, weight_decay=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict ={\n",
    "    'acc_train': [],\n",
    "    'acc_test': [],\n",
    "    'loss_train': [],\n",
    "    'loss_test': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    for epoch in range(100):\n",
    "\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        running_loss_val = 0\n",
    "        running_acc_val = 0\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        classifier.train()\n",
    "        dataset.set_split('train')\n",
    "        data_generator = DataLoader(dataset=dataset, batch_size=512, shuffle=True)\n",
    "        for batch_index, batch_dict in enumerate(data_generator, 1):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = classifier(batch_dict['x'])\n",
    "\n",
    "            loss_train = loss_func(y_pred, batch_dict['y'])\n",
    "            loss_item = loss_train.item()\n",
    "            running_loss += (loss_item - running_loss) / batch_index\n",
    "\n",
    "            loss_train.backward()\n",
    "\n",
    "            accuracy_score = compute_accuracy(batch_dict['y'], y_pred)\n",
    "            running_acc += (accuracy_score - running_acc) / batch_index\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        classifier.eval()\n",
    "        dataset.set_split('test')\n",
    "        data_generator = DataLoader(dataset=dataset, batch_size=512, shuffle=True)\n",
    "        for batch_index, batch_dict in enumerate(data_generator, 1):\n",
    "\n",
    "            y_pred = classifier(batch_dict['x'])\n",
    "\n",
    "            loss_train_val = loss_func(y_pred, batch_dict['y'])\n",
    "            loss_item_val = loss_train_val.item()\n",
    "            running_loss_val += (loss_item_val - running_loss_val) / batch_index\n",
    "\n",
    "            accuracy_score_val = compute_accuracy(batch_dict['y'], y_pred)\n",
    "            running_acc_val += (accuracy_score_val - running_acc_val) / batch_index\n",
    "\n",
    "        history_dict['acc_train'].append(running_acc)\n",
    "        history_dict['acc_test'].append(running_acc_val)\n",
    "        history_dict['loss_train'].append(running_loss)\n",
    "        history_dict['loss_test'].append(running_loss_val)\n",
    "\n",
    "        print(\"{:.2f} sec | epoch {} loss train: {:.2f} accuracy train: {:.2f} loss val {:.2f} accuracy val {:.2f}\".format(\n",
    "            time.time() - start, epoch, running_loss, running_acc, running_loss_val, running_acc_val\n",
    "        ))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"exit loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_dict['loss_train'])\n",
    "plt.plot(history_dict['loss_test'])\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.savefig('../reports/{}.png'.format(tanggal))\n",
    "\n",
    "pickle.dump(open(\"../reports{}.pkl\".format(tanggal), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
