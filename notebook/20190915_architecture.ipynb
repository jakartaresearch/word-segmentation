{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import hamming_loss\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanggal = datetime.datetime.today().strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pickle.load(open(os.path.join('../data/clean/data_clean_100k.res'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data['source_len']= d_data.source.str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data_selected = d_data[d_data['source_len'] == MAX_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_data_selected.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char = dict((idx, c) for idx, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\", 1))\n",
    "char2idx = dict((c, idx) for idx, c in enumerate(\"abcdefghijklmnopqrstuvwxyz\", 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2char[0] = '<UNK>'\n",
    "char2idx['<UNK>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_vectorizer(list_inputs, char_indices):\n",
    "    x = np.zeros((len([list_inputs]), MAX_LENGTH, len(char_indices)))\n",
    "    for i, input_ in enumerate([list_inputs]):\n",
    "        for t, char in enumerate(input_):\n",
    "            x[i, t, char_indices[char]] = 1\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_flag_space(sentence):\n",
    "    \n",
    "    no_space = []\n",
    "    flag_space = []\n",
    "    sentence = str(sentence)\n",
    "    for char in sentence: \n",
    "        if char != ' ':\n",
    "            no_space.append(char)\n",
    "            flag_space.append('0')\n",
    "        elif char == ' ':\n",
    "            flag_space[-1] = '1'\n",
    "            \n",
    "    no_space = ''.join(no_space)\n",
    "    flag_space = ''.join(flag_space)\n",
    "    \n",
    "    return flag_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_vectorizer(list_inputs, char_indices):\n",
    "    x = np.zeros((len([list_inputs]), MAX_LENGTH, len(char_indices)))\n",
    "    for i, input_ in enumerate([list_inputs]):\n",
    "        for t, char in enumerate(input_):\n",
    "            try:\n",
    "                x[i, t, char_indices[char]] = 1\n",
    "            except:\n",
    "                x[i, t, 0] = 1\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_space_to_list(flag):\n",
    "    return np.array(list(flag)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "d_data_selected['flag_space'] = d_data_selected['target'].apply(get_flag_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "d_data_selected.loc[:, 'matrix'] = d_data_selected.loc[:, 'source'].apply(char_vectorizer, args=(char2idx,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "d_data_selected['flag_space_array'] = d_data_selected.flag_space.apply(flag_space_to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andreas Chandra\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "d_data_selected['flag_space_sum'] = d_data_selected.flag_space_array.apply(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, data):\n",
    "        row, col = data.shape\n",
    "        train = data.loc[:int(row*.8)]\n",
    "        test = data.loc[int(row*.8):]\n",
    "        train.reset_index(drop=True, inplace=True)\n",
    "        test.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        self.lookup = {\n",
    "            'train': (train, len(train)),\n",
    "            'test': (test, len(test))\n",
    "        }\n",
    "        \n",
    "        self.set_split('train')\n",
    "        \n",
    "    def set_split(self, split = 'train'):\n",
    "        self.data, self.length = self.lookup[split]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        X = self.data.loc[index, 'matrix']\n",
    "        X = torch.Tensor(X).squeeze(0)\n",
    "        \n",
    "        y = np.array(list(self.data.loc[index, 'flag_space'])).astype(int)\n",
    "        y = torch.Tensor(y).squeeze(0)\n",
    "        \n",
    "        return {'x': X,\n",
    "               'y': y}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(len(char2idx), 256)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, input_, apply_sigmoid=False):\n",
    "        \n",
    "        y_pred, _ = self.lstm(input_)\n",
    "        y_pred, _ = self.lstm(input_, _)\n",
    "        y_pred = self.fc1(y_pred)\n",
    "        y_pred = self.fc2(y_pred)\n",
    "        y_pred = self.fc3(y_pred)\n",
    "        y_pred = self.fc4(y_pred)\n",
    "        y_pred = self.fc5(y_pred)\n",
    "        \n",
    "        if apply_sigmoid:\n",
    "            y_pred = torch.sigmoid(y_pred)\n",
    "        \n",
    "        y_pred = y_pred.squeeze(2)    \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred):\n",
    "    y_true = y_true.long().numpy()\n",
    "    y_pred = (y_pred > 0.5).long().numpy()\n",
    "    try:\n",
    "#         hamming_score = hamming_loss(y_true, y_pred)\n",
    "#         return 1 - hamming_score\n",
    "        return (y_true == y_pred).all(axis = 1).mean()\n",
    "    except:\n",
    "        print(\"y_true\", y_true, \"y_pred\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(d_data_selected)\n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(classifier.parameters(), lr = 0.001, weight_decay=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict ={\n",
    "    'acc_train': [],\n",
    "    'acc_test': [],\n",
    "    'loss_train': [],\n",
    "    'loss_test': []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/100 [00:00<?, ?it/s]\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "1it [00:08,  8.01s/it]\n",
      "\n",
      "2it [00:16,  8.03s/it]\n",
      "\n",
      "3it [00:24,  8.04s/it]\n",
      "\n",
      "4it [00:32,  8.05s/it]\n",
      "\n",
      "5it [00:40,  8.07s/it]\n",
      "\n",
      "6it [00:48,  8.02s/it]\n",
      "\n",
      "7it [00:56,  8.04s/it]\n",
      "\n",
      "8it [01:04,  8.05s/it]\n",
      "\n",
      "9it [01:13,  8.26s/it]\n",
      "\n",
      "10it [01:21,  8.32s/it]\n",
      "\n",
      "11it [01:30,  8.59s/it]\n",
      "\n",
      "12it [01:40,  8.84s/it]\n",
      "\n",
      "13it [01:50,  9.16s/it]\n",
      "\n",
      "14it [02:00,  9.46s/it]\n",
      "\n",
      "15it [02:09,  9.31s/it]\n",
      "\n",
      "16it [02:18,  9.29s/it]\n",
      "\n",
      "17it [02:28,  9.49s/it]\n",
      "\n",
      "18it [02:38,  9.62s/it]\n",
      "\n",
      "19it [02:48,  9.63s/it]\n",
      "\n",
      "20it [02:57,  9.68s/it]\n",
      "\n",
      "21it [03:06,  9.50s/it]\n",
      "\n",
      "22it [03:15,  9.28s/it]\n",
      "\n",
      "23it [03:24,  9.09s/it]\n",
      "\n",
      "24it [03:32,  8.92s/it]\n",
      "\n",
      "25it [03:41,  8.78s/it]\n",
      "\n",
      "26it [03:49,  8.60s/it]\n",
      "\n",
      "27it [03:57,  8.49s/it]\n",
      "\n",
      "28it [04:06,  8.45s/it]\n",
      "\n",
      "29it [04:14,  8.54s/it]\n",
      "\n",
      "30it [04:23,  8.56s/it]\n",
      "\n",
      "31it [04:32,  8.65s/it]\n",
      "\n",
      "32it [04:41,  8.75s/it]\n",
      "\n",
      "33it [04:49,  8.58s/it]\n",
      "\n",
      "34it [04:57,  8.54s/it]\n",
      "\n",
      "35it [05:06,  8.52s/it]\n",
      "\n",
      "36it [05:14,  8.40s/it]\n",
      "\n",
      "37it [05:22,  8.33s/it]\n",
      "\n",
      "38it [05:30,  8.27s/it]\n",
      "\n",
      "39it [05:39,  8.28s/it]\n",
      "\n",
      "40it [05:47,  8.27s/it]\n",
      "\n",
      "41it [05:55,  8.29s/it]\n",
      "\n",
      "42it [06:03,  8.23s/it]\n",
      "\n",
      "43it [06:12,  8.36s/it]\n",
      "\n",
      "44it [06:20,  8.30s/it]\n",
      "\n",
      "45it [06:28,  8.27s/it]\n",
      "\n",
      "46it [06:36,  8.17s/it]\n",
      "\n",
      "47it [06:45,  8.27s/it]\n",
      "\n",
      "48it [06:55,  8.81s/it]\n",
      "\n",
      "49it [07:05,  9.28s/it]\n",
      "\n",
      "50it [07:14,  9.07s/it]\n",
      "\n",
      "51it [07:23,  8.97s/it]\n",
      "\n",
      "52it [07:31,  8.85s/it]\n",
      "\n",
      "53it [07:40,  8.76s/it]\n",
      "\n",
      "54it [07:49,  8.80s/it]\n",
      "\n",
      "55it [07:57,  8.77s/it]\n",
      "\n",
      "56it [08:06,  8.70s/it]\n",
      "\n",
      "57it [08:14,  8.67s/it]\n",
      "\n",
      "58it [08:23,  8.61s/it]\n",
      "\n",
      "59it [08:32,  8.68s/it]\n",
      "\n",
      "60it [08:40,  8.71s/it]\n",
      "\n",
      "61it [08:48,  8.20s/it]\n",
      "\n",
      "62it [08:56,  8.18s/it]\n",
      "\n",
      "63it [09:03,  7.99s/it]\n",
      "\n",
      "64it [09:11,  8.06s/it]\n",
      "\n",
      "65it [09:21,  8.38s/it]\n",
      "\n",
      "66it [09:30,  8.66s/it]\n",
      "\n",
      "67it [09:39,  8.92s/it]\n",
      "\n",
      "68it [09:48,  8.95s/it]\n",
      "\n",
      "69it [09:57,  8.85s/it]\n",
      "\n",
      "70it [10:07,  9.20s/it]\n",
      "\n",
      "71it [10:16,  9.25s/it]\n",
      "\n",
      "72it [10:27,  9.68s/it]\n",
      "\n",
      "73it [10:37,  9.70s/it]\n",
      "\n",
      "74it [10:46,  9.66s/it]\n",
      "\n",
      "75it [10:56,  9.51s/it]\n",
      "\n",
      "76it [11:04,  9.15s/it]\n",
      "\n",
      "77it [11:13,  9.27s/it]\n",
      "\n",
      "78it [11:23,  9.51s/it]\n",
      "\n",
      "79it [11:34,  9.78s/it]\n",
      "\n",
      "80it [11:44,  9.77s/it]\n",
      "\n",
      "81it [11:54,  9.93s/it]\n",
      "\n",
      "82it [12:04,  9.98s/it]\n",
      "\n",
      "83it [12:14,  9.95s/it]\n",
      "\n",
      "84it [12:23,  9.83s/it]\n",
      "\n",
      "85it [12:34, 10.11s/it]\n",
      "\n",
      "86it [12:45, 10.37s/it]\n",
      "\n",
      "87it [12:56, 10.49s/it]\n",
      "\n",
      "88it [13:06, 10.45s/it]\n",
      "\n",
      "89it [13:17, 10.41s/it]\n",
      "\n",
      "90it [13:26, 10.23s/it]\n",
      "\n",
      "91it [13:37, 10.19s/it]\n",
      "\n",
      "92it [13:47, 10.17s/it]\n",
      "\n",
      "93it [13:57, 10.19s/it]\n",
      "\n",
      "94it [14:07, 10.06s/it]\n",
      "\n",
      "95it [14:16,  9.98s/it]\n",
      "\n",
      "96it [14:27, 10.04s/it]\n",
      "\n",
      "97it [14:37, 10.07s/it]\n",
      "\n",
      "98it [14:47, 10.03s/it]\n",
      "\n",
      "99it [14:57, 10.04s/it]\n",
      "\n",
      "100it [15:07, 10.06s/it]\n",
      "\n",
      "101it [15:17, 10.02s/it]\n",
      "\n",
      "102it [15:27, 10.17s/it]\n",
      "\n",
      "103it [15:37, 10.14s/it]\n",
      "\n",
      "104it [15:48, 10.16s/it]\n",
      "\n",
      "105it [15:58, 10.15s/it]\n",
      "\n",
      "106it [16:08, 10.13s/it]\n",
      "\n",
      "107it [16:19, 10.36s/it]\n",
      "\n",
      "108it [16:29, 10.47s/it]\n",
      "\n",
      "109it [16:40, 10.46s/it]\n",
      "\n",
      "110it [16:51, 10.63s/it]\n",
      "\n",
      "111it [17:03, 10.94s/it]\n",
      "\n",
      "112it [17:13, 10.82s/it]\n",
      "\n",
      "113it [17:24, 10.70s/it]\n",
      "\n",
      "114it [17:34, 10.54s/it]\n",
      "\n",
      "115it [17:44, 10.61s/it]\n",
      "\n",
      "116it [17:56, 10.74s/it]\n",
      "\n",
      "117it [18:06, 10.59s/it]\n",
      "\n",
      "118it [18:16, 10.55s/it]\n",
      "\n",
      "119it [18:26, 10.45s/it]\n",
      "\n",
      "120it [18:36, 10.33s/it]\n",
      "\n",
      "121it [18:47, 10.24s/it]\n",
      "\n",
      "122it [18:57, 10.19s/it]\n",
      "\n",
      "123it [19:07, 10.23s/it]\n",
      "\n",
      "124it [19:17, 10.20s/it]\n",
      "\n",
      "125it [19:28, 10.32s/it]\n",
      "\n",
      "126it [19:38, 10.45s/it]\n",
      "\n",
      "127it [19:49, 10.44s/it]\n",
      "\n",
      "128it [19:59, 10.35s/it]\n",
      "\n",
      "129it [20:09, 10.24s/it]\n",
      "\n",
      "130it [20:19, 10.25s/it]\n",
      "\n",
      "131it [20:31, 10.60s/it]\n",
      "\n",
      "132it [20:42, 10.76s/it]\n",
      "\n",
      "133it [20:53, 10.76s/it]\n",
      "\n",
      "134it [21:03, 10.72s/it]\n",
      "\n",
      "135it [21:14, 10.78s/it]\n",
      "\n",
      "136it [21:26, 10.98s/it]\n",
      "\n",
      "137it [21:36, 10.88s/it]\n",
      "\n",
      "138it [21:46, 10.62s/it]\n",
      "\n",
      "139it [21:56, 10.45s/it]\n",
      "\n",
      "140it [22:07, 10.41s/it]\n",
      "\n",
      "141it [22:17, 10.35s/it]\n",
      "\n",
      "142it [22:27, 10.18s/it]\n",
      "\n",
      "143it [22:36,  9.98s/it]\n",
      "\n",
      "144it [22:46, 10.07s/it]\n",
      "\n",
      "145it [22:56, 10.05s/it]\n",
      "\n",
      "146it [23:06,  9.97s/it]\n",
      "\n",
      "147it [23:16,  9.86s/it]\n",
      "\n",
      "148it [23:25,  9.73s/it]\n",
      "\n",
      "149it [23:34,  9.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exit loop\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in tqdm(range(100)):\n",
    "\n",
    "        running_loss = 0\n",
    "        running_acc = 0\n",
    "        running_loss_val = 0\n",
    "        running_acc_val = 0\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        classifier.train()\n",
    "        dataset.set_split('train')\n",
    "        data_generator = DataLoader(dataset=dataset, batch_size=512, shuffle=True)\n",
    "        for batch_index, batch_dict in tqdm(enumerate(data_generator, 1)):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred = classifier(batch_dict['x'])\n",
    "\n",
    "            loss_train = loss_func(y_pred, batch_dict['y'])\n",
    "            loss_item = loss_train.item()\n",
    "            running_loss += (loss_item - running_loss) / batch_index\n",
    "\n",
    "            loss_train.backward()\n",
    "\n",
    "            accuracy_score = compute_accuracy(batch_dict['y'], y_pred)\n",
    "            running_acc += (accuracy_score - running_acc) / batch_index\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        classifier.eval()\n",
    "        dataset.set_split('test')\n",
    "        data_generator = DataLoader(dataset=dataset, batch_size=512, shuffle=True)\n",
    "        for batch_index, batch_dict in tqdm(enumerate(data_generator, 1)):\n",
    "\n",
    "            y_pred = classifier(batch_dict['x'])\n",
    "\n",
    "            loss_train_val = loss_func(y_pred, batch_dict['y'])\n",
    "            loss_item_val = loss_train_val.item()\n",
    "            running_loss_val += (loss_item_val - running_loss_val) / batch_index\n",
    "\n",
    "            accuracy_score_val = compute_accuracy(batch_dict['y'], y_pred)\n",
    "            running_acc_val += (accuracy_score_val - running_acc_val) / batch_index\n",
    "\n",
    "        history_dict['acc_train'].append(running_acc)\n",
    "        history_dict['acc_test'].append(running_acc_val)\n",
    "        history_dict['loss_train'].append(running_loss)\n",
    "        history_dict['loss_test'].append(running_loss_val)\n",
    "\n",
    "        print(\"{:.2f} sec | epoch {} loss train: {:.2f} accuracy train: {:.2f} loss val {:.2f} accuracy val {:.2f}\".format(\n",
    "            time.time() - start, epoch, running_loss, running_acc, running_loss_val, running_acc_val\n",
    "        ))\n",
    "except KeyboardInterrupt:\n",
    "    print(\"exit loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_dict['loss_train'])\n",
    "plt.plot(history_dict['loss_test'])\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.savefig('../reports/{}.png'.format(tanggal))\n",
    "\n",
    "pickle.dump(open(\"../reports{}.pkl\".format(tanggal), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
